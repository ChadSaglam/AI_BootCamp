{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9REoP_tA-rNH"
      },
      "outputs": [],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "import collections\n",
        "\n",
        "sc = SparkContext()\n",
        "rdd = sc.parallelize([3,4,56,7,4,2])\n",
        "\n",
        "sq = rdd.map(lambda x:x*x)\n",
        "print(sq.collect())\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "I8X5wWjv_HlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ratings Histogram"
      ],
      "metadata": {
        "id": "1bXpWOYfSukt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "import collections\n",
        "\n",
        "# Other computer entegration with Ratings Histogram name\n",
        "conf = SparkConf().setMaster('local').setAppName('RatingsHistogram')\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "lines=sc.textFile('u.data')\n",
        "\n",
        "ratings=lines.map(lambda x:x.split()[2])\n",
        "result=ratings.countByValue()\n",
        "sortedResults=collections.OrderedDict(sorted(result.items()))\n",
        "\n",
        "for key,value in sortedResults.items():\n",
        "  print(key, value)\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "3i3zYaRyP4wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Minimum Temperature"
      ],
      "metadata": {
        "id": "uU2t856YdBPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"MinTemperatures\")\n",
        "sc = SparkContext(conf = conf)\n",
        "\n",
        "def parseLine(line):\n",
        "  fields = line.split(',')\n",
        "  stationID = fields[0]\n",
        "  entryType = fields[2]\n",
        "  temperature = float(fields[3]) * 0.1 * (9.0 / 5.0) + 32.0\n",
        "  return (stationID, entryType, temperature)\n",
        "\n",
        "lines = sc.textFile(\"1800.csv\")\n",
        "parsedLines = lines.map(parseLine)\n",
        "#minTemps = parsedLines.filter(lambda x: \"TMIN\" in x[1])\n",
        "maxTemps = parsedLines.filter(lambda x: \"TMAX\" in x[1])\n",
        "#stationTemps = minTemps.map(lambda x: (x[0], x[2]))\n",
        "stationTemps = maxTemps.map(lambda x: (x[0], x[2]))\n",
        "#minTemps = stationTemps.reduceByKey(lambda x, y: min(x,y))\n",
        "maxTemps = stationTemps.reduceByKey(lambda x, y: max(x,y))\n",
        "#results = minTemps.collect();\n",
        "results = maxTemps.collect();\n",
        "\n",
        "for result in results:\n",
        "  print(result[0] + \"\\t{:.2f}F\".format(result[1]))\n",
        "\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "35p9UIFdaTOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word Count"
      ],
      "metadata": {
        "id": "iSPoV1AIgmD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf().setMaster (\"local\").setAppName (\"Wordcount\")\n",
        "sc = SparkContext(conf = conf)\n",
        "\n",
        "input=sc.textFile('book.txt')\n",
        "words=input.flatMap(lambda x: x.split())\n",
        "wordCounts=words.countByValue()\n",
        "\n",
        "for word, count in wordCounts.items() :\n",
        "  print(word, count)\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "4giDQuzhgriG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "def normalizeWords(text):\n",
        "  return re.compile(r'\\W+', re.UNICODE).split(text.lower())\n",
        "\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"WordCount\")\n",
        "sc = SparkContext(conf = conf)\n",
        "\n",
        "input = sc.textFile(\"book.txt\")\n",
        "words = input.flatMap(normalizeWords)\n",
        "\n",
        "wordCounts = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
        "wordCountsSorted = wordCounts.map(lambda x: (x[1], x[0])).sortByKey(False)\n",
        "results = wordCountsSorted.collect()\n",
        "\n",
        "for result in results:\n",
        "  count = str(result[0])\n",
        "  word = result[1].encode('ascii', 'ignore')\n",
        "  if(word):\n",
        "    print(word.decode() + \":\\t\\t\" + count)\n",
        "\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "KvOeD9klejdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"PopularHero\")\n",
        "sc = SparkContext(conf = conf)\n",
        "\n",
        "\n",
        "def countCoOccurences(line):\n",
        "  elements = line.split()\n",
        "  return(int(elements[0]), len(elements) - 1)\n",
        "\n",
        "\n",
        "def parseNames(line):\n",
        "  fields = line.split('\\\"')\n",
        "  return (int(fields[0]), fields[1].encode(\"utf8\"))\n",
        "\n",
        "names = sc.textFile(\"Marvel-names.txt\")\n",
        "namesRdd = names.map(parseNames)\n",
        "\n",
        "lines = sc.textFile(\"Marvel-graph.txt\")\n",
        "\n",
        "pairings = lines.map(countCoOccurences)\n",
        "totalFriendsByCharacter = pairings.reduceByKey(lambda x, y : x + y)\n",
        "flipped = totalFriendsByCharacter.map(lambda xy : (xy[1], xy[0]))\n",
        "\n",
        "mostPopular = flipped.max()\n",
        "mostPopularName = namesRdd.lookup(mostPopular[1])[0]\n",
        "\n",
        "print(str(mostPopularName) + \" is the most popular superhero, with \" + str(mostPopular[0]) + \\\n",
        "      \" co-appearances.\")\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "mIjFPvTbiM9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pima Indians Diabetes"
      ],
      "metadata": {
        "id": "b2O9rBscliLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Step 1: Initialize Spark\n",
        "spark = SparkSession.builder.appName(\"PimaIndianclassification\").getOrCreate()\n",
        "\n",
        "# Step 2: Load the dataset\n",
        "data = spark.read.csv(\"pima-indians-diabetes.csv\", inferSchema=True, header=True)\n",
        "\n",
        "# Step 3: Prepare the data for training\n",
        "feature_columns = data.columns [: -1]\n",
        "assembler = VectorAssembler (inputCols=feature_columns, outputCol=\"features\")\n",
        "data = assembler.transform(data).select(\"features\", \"Outcome\")\n",
        "\n",
        "# step 4: Split the data into training and testing sets\n",
        "train_data, test_data = data.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "# Step 5: Train a logistic regression model\n",
        "lr = LogisticRegression (labelCol=\"Outcome\", featuresCol=\"features\")\n",
        "model = lr.fit(train_data)\n",
        "\n",
        "# Step 6: Make predictions on the test data\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "# Step 7: Evaluate the model\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"Outcome\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "LjeLcgsRiNHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eChsfBv1iNKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "WKRpnu21aI4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cYOchxy8iNM7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}